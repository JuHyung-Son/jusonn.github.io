---
title: Glove 논문 리뷰
date: 2018-04-30
author: JuHyung Son
layout: post
tags:
  - NLP
  - Word embedding
  - Glove
categories:
  - Deep_Learning
---

# Introduction
단어를 벡터로 표현하는 방법엔 크게 Matrix factorization과 local context window methods가 있습니다. 전자는 LSA(latent semantic analysis)와 같은 방법에 쓰이고 후자는 이전에 본 skip gram model에 쓰이는 방법이죠. 두 방법은 어떤 것이 더 좋은 것이다라고 하기엔 서로의 장단점이 정 반대입니다. skip gram과 같은 모델은 단어를 표현하고 유추하는데 좋은 성능을 보이지만, 한 문장의 window 단위로 학습을 하기 때문에 전체적인 말뭉치의 통계적 정보 나타내는데는 별로입니다. 이 co-occurrence statistics 정보가 자연어 처리에 얼마나 중요한지는 아직 모르겠지만, 이것이 큰 단점으로 작용한다고 합니다. 결과에 대한 설명력이 부족한건가 싶기도 하고요. 반면 LSA와 같은 모델은 통계적 정보를 효율적으로 나타내지만 단어를 표현하는데에는 좋지 않고 중간에 SVD를 하는 과정에서 계산량이 굉장히 많기도 합니다.
Matrix factorization 방법은 global 하게 단어를 표현하지만 skip gram 모델은 학습의 특성 때문에 local하게 단어를 학습하며 표현하죠.

저자는 이 두 방법의 장점만을 차용할 모델을 생각해 냅니다. 바로 Glove, Global vector 죠. Matrix factorization의 global한 co-occurrence statistics 정보와 skip gram의 vector 표현을 가져 온 모델입니다. 결과적으로 word-word co-occurrence 행렬에서 nonzero인 값에서만 효율적으로 학습을 하여 통계 정보를 잘 활용하며 기존의 모델보다 좋은 성능을 보이는 단어들의 의미있는 벡터 공간도 만들어낸다고 합니다.

<a href="https://nlp.stanford.edu/projects/glove/"> 스탠포드 Glove 바로가기</a>

# Objects

그래서 Glove의 목표는 word-word co-occurrence 행렬을 사용하여 효율적인 학습을 하는 모델입니다. 그리고 일반적인 학습처럼 least square model로 loss를 평가하며 모델의 특성상 weighted least squares를 사용한다고 합니다.

# Model

모델을 설계하는 과정에서 해결해야 할 가장 큰 문제는

1. 어떻게 co-occurrence 행렬에서 어떤 의미를 뽑아내느냐
2. 생성된 단어 벡터가 어떻게 그 의미를 나타내느냐

였다고 합니다. 그리고 이 두 질문에 대한 답이 global vector, global corpus statistics이고요.
모델에 쓰이는 표기들은 몇가지 없습니다.

- $X$: 단어에서 단어로 가는 co-occurrence 행렬
- $X_{ij}$: 단어 i의 문맥에서 단어 j가 나타나는 횟수
- $X_{i} : \sum_{k}X_{ik}$: 단어 i의 문맥에서 나타나는 모든 단어들의 갯수
- $P_{ij}=P(j\|i)=X_{j}/X_{i}$: 단어 i의 문맥에서 단어 j가 나타날 확률

<div align="center"> <img src="/image/glove/2.png" /> </div>

논문에서 간단히 소개하는 예시를 봅시다. 먼저 k가 solid일 때, ice의 문맥에서 solid가 나타날 확률은 steam일 때보다 약 10배 정도 높죠. 얼음이 고체이니 자연스럽게 보입니다.

반대로 k가 gas일 경우엔 steam의 문맥에서 gas가 나타낼 확률이 10정도 높습니다. 그래서 $P(k\|ice)/P(k\|steam)$ 은 전자의 경우 1 보다 매우 높은 값이 나오고 후자의 경우 1보다 매우 작은 값이 나오게 되죠.

k가 water일 때는 ice와 steam 두 경우 모두 비슷하게 나올 것 같습니다. 그래서 $P(k\|ice)$ 와 $P(k\|steam)$ 은 비슷한 값을 가지게 되고 이 둘을 나누면 1에 가까운 값이 나오죠. 반대로 k가 fashion일 때는 두 경우 모두 잘 나오지 않을겁니다. 그러니 둘을 나누면 역시 1에 가까운 값이 나오게 되죠.

얼핏 보면, co-occurrence의 비율로 단어 벡터를 학습하는 것이 잘 작동할 것으로 보이고 여기서 glove 의 loss 함수 모델링이 시작됩니다.

위의 예시에서 $\frac{P_{ik}}{P_{jk}}$ 가 단어를 잘 설명할 것으로 보이니 이러한 모양을 가지는 loss 함수 F를 단어 $w_{i}, w_{j}, \tilde{w}_{k}$ 를 파라미터로 가지는 함수로 설정합니다. 아직 모르지만 모델링을 통해 찾을 어떤 임의의 함수이죠. 그리고 $w \in \mathbb{R}^{d}, \tilde{w} \in \mathbb{R}^{d}$ 단어 벡터입니다.

여기서 $\tilde{w}_k$는 타겟 단어에 대한 단어 벡터입니다. $W$와 $\tilde{W}$ 똑같은 모양의 벡터이고 학습시 웨이트의 초기값만 다릅니다. 이런 방식으로 따로 학습을 하고 나중에 합치는 것이 오버피팅도 줄이고 성능도 좋다고 합니다. 학습을 마치고 나서는 이 두 벡터를 단순하게 더한 것을 최종 웨이트로 사용합니다. 단순히 이 방식이 성능이 더 좋기 때문에 사용했다고 하네요. 아무튼 밑과 같이 함수 F를 정해봅니다.


$$F(w_i, w_j, \tilde{w}_k) = \frac{P_{ik}}{P_{jk}}$$


함수 F가 세 파라미터를 사용해 $\frac{P_{ik}}{P_{jk}}$ 를 가장 잘 표현하길 바라는 겁니다. 여기서 $w_{i}, w_{j}$ 가 벡터이므로 두 단어 모두를 파라미터로 갖기 보다는 이 두 단어의 차이를 파라미터로 가지는 편이 편리하고 합리적이기도 하겠죠. 이렇게 말입니다.

$$F(w_{i} - w_{j} , \tilde{w}_k) = \frac{P_{ik}}{P_{jk}}$$

하지만 F의 파라미터들은 벡터이고 오른쪽의 값은 스칼라입니다. 이 말은 F라는 함수가 복잡한 함수일 것이라는 거죠. 이것을 피하기 위해 파라미터들을 스칼라곱 해줍니다.

$$F((w_{i}-w_{j})^{T} \tilde{w}_{k}) = \frac{P_{ik}}{P_{jk}}$$

$\tilde{w}$ 와 $w$ 를 생각해보면 타겟 단어(row)와 단어(column)는 학습을 하며 그 역할이 계속 바뀌는 관계입니다.($X_{ij}$ 의 i가 타겟 단어, j가 단어입니다.) 단어(column)가 타겟 단어(row)가 되기도 하고 그냥 단어(column)으로 쓰이기도 하는 것처럼요. 그렇게 되려면 $\tilde{w}$ 와 $w$ 는 똑같은 모양일 수 밖에 없죠. 그리고 이 말은 $X$ 는 symmetric하다는 이야기입니다. $X$ 와 $X^{T}$ 는 같다는 얘기죠. 그런데 $X$ 는 symmetric 하지 않기 때문에 추가적인 모델링이 필요합니다.

먼저 함수 F가 $(\mathbb{R} , +), (\mathbb{R}_{>0}, *)$ 에 대해 **homomorphism** 해야 합니다.

homomorphism은 두 group의 구조의 비슷함을 보는 방법으로 현대대수에서 배우는 개념입니다.

homomorphism은 각각의 group의 구조를 그대로 따르는 함수로, 서로 다른 연산을 가지는 두 다른 그룹 G,H에 대해서 함수 f가 $f:G \rightarrow H$ 인 함수 일 때, $x,y \in G$ 이고 $xy=z$ 라면 $f(x)f(y)=f(z) \rightarrow f(x)f(y)=f(xy)$ 를 만족하는 함수 f입니다.

그리고 이 homomorphism의 대표적인 예가 바로 exp 함수죠. $e^{x+y} = e^{x}e^{y}$ 를 보면 쉽게 이해가 됩니다.

그래서 homomorphism을 만족하도록 다음처럼 식을 변형해줍니다.

$$F((w_{i}-w_{j})^{T} \tilde{w}_{k}) = \frac{F(w_{i}^{T} \tilde{w}_{k})}{F(w_{j}^{T} \tilde{w}_{k})}~~~~~~~~~~~(4)$$

그리고 정의에 따라 다음처럼 표현됩니다.

$$F(w_{i}^{T} \tilde{w}_{k}) = P_{ik} = \frac{X_{ik}}{X_{i}}$$

(4)에 대한 해답으로 $F=exp$ 또는

$$w_{i}^{T} \tilde{w}_{k}= log(P_{ik})=log(X_{ik})-log(X_{i})$$

를 얻게 됩니다. 위의 모든 조건을 만족하죠.

이렇게 얻어진 식

$$w_{i}^{T} \tilde{w}_{k} + log(X_{i}) = log(X_{ik})$$

에서 $log(X_{i})$ 는 k와는 independent 합니다. 따라서 bias $b_{i}$ 로 표현할 수 있고 이에 따라 $\tilde{b}_{i}$ 도 더해줍니다.
결과적으로 다음의 식을 얻게 됩니다.

$$w_{i}^{T} \tilde{w}_{k} + b_{i} + \tilde{b}_{i} = log(X_{ik})$$

이렇게 loss 함수 모델이 완성되었지만 이 모델은 모든 co-occurrence에 대해 같은 웨이트를 준다는 것입니다. 아주 자주 등장하거나 아주 드문 단어에 대해서도요. 저자는 새로운 함수 $f(X_{ij})$ 를 도입해서 이 문제를 해결합니다. 드문 단어에 대해 웨이트를 적게 주는 함수입니다.

$$f(X_{ij}) = (x/x_{max})^{\alpha} ~~when~~ x \lt x_{max}, ~~~1 ~~ Otherwise$$

마지막으로 저자가 제시하는 loss 함수는 이렇습니다. Least squre의 형태를 띄고 함수 f를 이용해 웨이트를 준 모습입니다.

$$J = \sum^{V}_{i,j=1} f(X_{ij})(w^{T}_{i} \tilde{w}_{j} + b_{i} + \tilde{b}_{j} - log X_{ij})^{2}$$

glove로 임베딩한 결과는 저자에 의하면 몇몇 말뭉치에 대해선 word2vec보다 좋은 성능을 보이며 필요한 계산량도 더 적다고 합니다.

<div align="center"><img src='/image/glove/1.png' width=600px/> </div>

저자의 실험에 의하면 glove도 위와 같이 단어들 간의 관계 (semantic)을 잘 나타냅니다.

<div align="center"><img src='/image/glove/3.png' width=600px/> </div>

또 glvoe는 rare한 단어들에 대해서도 좋은 성능을 보이는데요. 위 개구리와 비슷한 단어들은 얼핏 보면 뭔지 모르겠지만, 다 결국 개구리입니다. 이런 점이 glove의 특장점이지 않을까 싶은데요.

이렇게 glove에 대하여 알아봤습니다. 몇몇 커뮤니티에서 들은바로는 glove가 생각보다 성능이 좋지 않다는 이야기도 있고 한국어에 대해서는 성능이 좋다는 얘기도 있습니다. 하지만 역시 카더라는 소문이고 역시 직접 말뭉치에 적용해봐야하지 않을까 싶습니다.
