---
title: Bag of words 보기
date: 2018-05-02
author: JuHyung Son
layout: post
tags:
  - NLP
  - bow
categories:
  - Deep_Learning
---
Bag of words(BOW)는 텍스트 데이터를 표현하는 방법 중 하나로 기계학습에서 예전부터 사용되던 방법입니다. 자연어 처리에 입문한지 얼마되지 않아 벡터 방식의 임베딩 기법을 먼저 배웠지만, BOW 방식은 여전히 언어 모델링, 문서 분류에 여전히 잘 작동하고 있고 실제로도 많이 쓰이는 방법입니다. 왜 BOW를 써야하는지, 또 어떻게 쓰이고 있는지 한번 보겠습니다.

# 텍스트

기계학습 모델을 만들다보면 항상 해주는 것이 데이터를 모델에 맞게 변형시켜주는 작업입니다. 예를 들어 부동산 가격 예측 모델링을 한다면, 숫자로 된 값들을 nomalize 해주고 카테고리 변수는 dummy 변수로 만들어 컴퓨터가 카테고리를 이해하게 해주는 작업들 말입니다. 또 이미지를 불러 올 때는 행렬로 불러오죠.

이런 작업들은 모두 알고리즘이 데이터를 받아들일 수 있게 하려는 작업입니다. 텍스트 역시 텍스트를 그대로 받아들이는 알고리즘은 없습니다. 텍스트 역시 이미지처럼 어떤 숫자로 변환되어야 하죠. 하지만 텍스트는 일단 언어가 제각기 다르고 이미지처럼 컴퓨터 자체에서 어떤 행렬로 받아들여지는 것이 아닙니다. 그래서 이미지와 다르게 텍스트 자체를 어떻게 숫자화할 지 부터 시작해야하는 것입니다.

## BOW

그 방법 중 하나인 BOW는 텍스트에 나타나는 단어들의 갯수를 이용하는 방법이죠. 단어들을 가방에 넣는겁니다. 하지만 큰 가방에 단어들을 넣으면 단어들의 순서나 문장에서 나타나는 구조들을 유지할 수 없습니다. 단지 텍스트의 단어들이 몇 번 나타나는지만 고려할 수 있죠. BOW가 바로 그런 방법입니다.

BOW는 특히 문서 분류에 좋은 성능을 보입니다. 그럴 수 밖에 없는 것이 비슷한 문서에는 비슷한 구조의 BOW가 나타날 것이니까요.

```
I am studying deep learning.
I am writing a post.
My post is about deep learning.
```
이 세 문장으로 BOW를 만들어 봅시다. 이 문장들에서 나타나는 단어들을 모아보면,
```
I
am
studying
deep
learning
writing
a
post
My
is
about
```
입니다. 이것을 이용해 위 세문장을 binary vector로 표현하는 것이 BOW입니다.
```
I am studying deep learning = [1,1,1,1,1,0,0,0,0,0,0]
I am writing a post = [1,1,0,0,0,1,1,1,0,0,0]
My post is about deep learning = [0,0,0,1,1,0,0,1,1,1,1]
```

만약 이 가방에 들어있지 않은 단어가 포함된 문장이 있어도 BOW는 그 없는 단어를 제외하고 있는 단어만을 가지고 텍스트를 벡터로 만들 것입니다.

BOW는 단점은 모델이 너무나 심플하다는 것입니다.

문서가 한 권의 책 정도로 엄청나게 커진다면, 6글자 정도되는 한 문장의 텍스트를 표현하기 위해 수천 사이즈의 벡터가 필요하게 되고 이는 당연히 비효율적이죠.

거의 대부분의 값이 0인 이런 sparse vector는 계산량으로나 공간적으로나 치명적인 단점으로 작용합니다. 또 언어라는 특성상 단어의 중요성과 상관없이 빈도수가 제각각인데 이런 것을 고려하지 않는다는 단점도 있습니다. 이런 단점을 약간이나 덜기 위해 하는 과정들이 있습니다.

- 모든 텍스트를 소문자로 만들기
- 기호 없애기
- 자주 나타나지만 의미는 별로 갖지 않는 단어 없애기
- 오탈자 수정
- stemming 하기 ("컴퓨터들" -> "컴퓨터")

물론 이 방법은 문제의 근본적인 해결방법은 아닙니다. 또 이 과정은 거의 모든 자연어 처리에 들어가는 과정이기도 하죠.

## Gram

BOW를 조금 더 개선시켜서 이번엔 단어 하나만을 보는 것이 아니라 주변의 n개 단어를 뭉쳐서 봅니다. 뭉쳐진 n개의 단어들을 gram이라고 합니다. ~~노트북 아님~~ 이 n개의 단어 쌍들은 보통 텍스트의 의미를 좀 더 잘 잡아낸다고 합니다.

2개의 단어를 묶어서 사용하면 bi-gram model, 3개의 단어를 묶으면 tri-gram model이라고 부릅니다.

위의 예시 문장 중 첫 문장으로 bow를 만들어 봅시다.

`I am studying deep learning`
```
i am
am studying
studying deep
deep learning
```

> a bag-of-bigrams representation is much more powerful than bag-of-words, and in many cases proves very hard to beat.
> -- *Neural Network moethods in natural language processing*

## Limitations

BOW는 아주 심플한 만큼 그 한계도 명확합니다.

- 단어 전처리가 확실해야 합니다. 단어간의 관계들이 고려되지 않기 때문에 "초콜렛","초코릿" 과 같은 단어를 엄밀히 정제해놓아야 하죠. 만일 텍스트가 뉴스와 같은 텍스트가 아닌 댓글, 트위터라면 거의 불가능한 작업입니다.
- BOW로 만들어진 행렬은 매우 sparse합니다. 이건 계산적으로나 공간적으로나 모델을 사용하기 어렵게 만들죠.
- 단어들간의 관계를 전혀 고려하지 못합니다. 벡터를 이용한 모델에 비하면 아주 큰 단점입니다.

다음엔 BOW를 사용하여 실제 텍스트 데이터를 모델링 해봅니다.

### 더 읽기

<a href="https://en.wikipedia.org/wiki/N-gram"> Bag of words</a>
