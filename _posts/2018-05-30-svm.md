---
title: SVM과 Kernel 보기
date: 2018-05-30
author: JuHyung Son
layout: post
tags:
  - SVM
categories:
  - Machine_Learning
---

# Intro

SVM은 기본적으로 지도 학습의 한 알고리즘으로 Classification과 Regression 모두 가능한 알고리즘입니다. 1963년에 Vladimir N. Vapnik, Alexey Ya. Chervonenkis가 개발한 이후로 꾸준히 발전된 모델이고 한때는 기계학습을 대표하는 알고리즘이었습니다.

Backpropagation의 등장으로 인공신경망 모델이 다시 뜨면서 한물간? 알고리즘이지만 여전히 SVM을 볼 가치는 충분하죠. 딥러닝이 여전히 대세이긴 하지만, 대부분의 많은 산업에선 여전히 SVM이 강력한 알고리즘입니다. 딥러닝에 비해 필요한 데이터가 매우 적기도 하며 가장 최적의 결과를 보인다는 것도 밝혀졌으니까요.

# SVM

SVM이 기본적으로 하고자 하는 건 decision boundary을 가장 잘 적용하려는 것입니다. 아래와 두개의 클래스를 가진 데이터를 분류한다고 해봅시다. 데이터는 간단하니 linearly separation이 가능해 보입니다. 즉 선은 두 클래스 사이에 그어 분류를 하는건데 그릴 수 있는 선은 매우 많죠. svm은 여기서 가장 좋은 선은 찾아 긋는 알고리즘입니다. SVM은 가장 좋은 선은 두 클래스 사이를 반으로 나누는 것이 가장 좋다고 가정합니다.

그렇다면 아래의 두 점선은 좋은 분류 기준이 아닌거 같죠.

<div align="center"> <img src="/image/svm/1.png" /> </div>

아래의 선이 가장 좋은 분류 기준이 됩니다. 이 직선을 수식으로 나타내면 이렇습니다. $\overrightarrow{w}^{T} \overrightarrow{x}+b = 0$ 이 직선은 기준으로 직선보다 위에 있으면 class 2 아래 있으면 class 1 으로 보는거죠. 일정거리는 직선에서 가장 가까운 데이터입니다. 점선에 걸쳐있는 점이 바로 그것이고 이것을 support vector 라고 부릅니다. 그래서 두 점선 사이의 거리를 m, margin 이라고 합니다. 그러면 이 margin이 최대일 때, 즉 두 점선 사이의 거리가 가장 클 때가 직선이 가장 좋은 분류 기준이 되는 것이죠.

이제 이것을 수식으로 봅시다.

class 2를 positive $x_+$, class 1을 negative $x_-$ 이라고 보면 각 데이터는 이렇게 표현할 수 있습니다.

$$y_i = +1 ~ when ~ \overrightarrow{w} \overrightarrow{x_+} + b \geq 1$$

$$y_i = -1 ~ when ~ \overrightarrow{w} \overrightarrow{x_-} + b \leq -1$$

그런데 사실 위의 두 식은 같습니다! 아래의 식을 한 번 보죠.

$$y_i (\overrightarrow{w} x_i + b) \geq 1$$

각 데이터를 표현한 식 양쪽에 $y_i$ 를 곱해준 식입니다. 오른쪽 항은 $y_i$ 대신 1, -1을 곱해주었죠. 계산을 해본다면 positive 일 때와 negative 일 때 모두 위의 식이 나옵니다. 그러면 SVM은 위의 식만 만족하는 분류 기준을 찾으면 되는 것입니다.

이번엔 margin 을 어떻게 최대화할까요? Margin 은 두 점선 사이의 거리라고 하였습니다. 가장 가까워 보이는 각 클래스의 점들 사이의 거리죠. 그러면 각 두 점 사이의 거리를 다음처럼 구해봅니다. 직선에 수직인 벡터를 $\overrightarrow{w}$ 라고 하면 두 점선 사이의 거리인 margin은 다음과 같습니다.

$$margin = (x_+ - x_-) \cdot \frac{\overrightarrow{w}}{||w||} = \frac{2}{||w||}$$

<div align="center"> <img src="/image/svm/3.png" /> </div>

<div>

</div>

이 margin이 최대가 되는 점이 바로 support vector 가 되는 것입니다. $max ~ \frac{2}{||w||}$ 와 같은 형태는 최대값을 구하기 어렵습니다. 그래서 $min ||w||$ 와 같은 형태로 최소값을 구하는 형태로 바꾸죠. 그리고 수학적인 편리를 위해  $min \frac{1}{2}||w|| ^2$ 을 구하는 문제로 바꿔줍니다.

이제 SVM은 이런 최적화 문제로 바뀌었습니다.

$$minimize ~ \frac{1}{2} ||w||^{2}$$
$$subject ~ to ~ 1- y_i (\overrightarrow{w} \overrightarrow{x_i} + b) \leq 0$$

이 최적화 문제는 답을 구하는 방법이 이미 밝혀졌죠. 이 최적화 문제는 lagrangian 식의 gradient 가 0인 것이 최적화 값입니다. lagrangian 은 $f(x) + \sum \alpha_i g_i(x)$ 의 형태를 띄는 식으로 $\alpha$ 는 lagrangian multiplier 입니다. 위의 해답을 svm의 문제에 적용하면, 먼저 lagrangian 은

$$L = \frac{1}{2} w^{T} w + \sum \alpha_i (1-y_i (\overrightarrow{w} \overrightarrow{x_i} + b))$$

Loss 가 아닌 Lagrangian 입니다. 이제 lagrangian의 $w, b$ 에 대한 gradient를 구해봅니다. 이 둘이 모두 0이 되어야 하죠.

$$\frac{\partial L}{\partial w} = \overrightarrow{w} - \sum \alpha_i y_i x_i = 0 \rightarrow \overrightarrow{w} = \sum \alpha_i y_i x_i$$

$$\frac{\partial L}{\partial b} = - \sum \alpha_i y_i = 0$$

위의 편미분을 통해 $\overrightarrow{w}$ 와 $\sum \alpha y$을 알아내었고 이것을 기존의 lagrangian에 대입시키면 다음의 결과를 얻습니다.

$$L = - \frac{1}{2} \sum_j \sum_i \alpha_i \alpha_j y_i y_j x_i^T x_j + \sum \alpha_i$$

이제 lagrangian 은 $\alpha$ 에 대한 함수라는 걸 알았습니다. 그리고 이러한 문제는 Dual problem 으로 알려진 문제입니다. dual problem은 간단히 말해 우리가 $\overrightarrow{w}$ 을 안다면 모든 $\alpha_i$ 를 아는 것이고 모든 $\alpha_i$ 를 안다면 $\overrightarrow{w}$ 를 안다는 것입니다. w 는 $\alpha$ 값에 의해 정해지니 그렇겠죠? 그래서 dual problem 은 다음처럼 정의됩니다.

$$max ~ W(\alpha) = \sum \alpha_i - \frac{1}{2} \sum_i \sum_j \alpha_i \alpha_j y_i y_j x_i^T x_j$$

$$subject ~ to ~ \alpha_i \geq 0 , ~ \sum \alpha_i y_i$$

이것은 또 quadratic programming problem 이고 이것은 $\alpha_i$ global maximum 을 항상 찾을 수 있다고 알려져 있습니다. Global maximum 을 항상 찾을 수 있다니 너무 좋죠. 따로 소프트웨어를 이용해 quadratic programming 을 풀면 밑의 그림과 같이 $\alpha_i$ 를 찾을 수 있죠.

<div align="center"> <img src="/image/svm/4.png" /> </div>

그림을 보면 대부분의 데이터에서 $\alpha$ 는 0이고 점선에 걸친 데이터에서만 $\alpha$ 가 0이 아니죠. 즉 $\alpha \neq 0$ 인 데이터가 바로 support vector 입니다.

이렇게 해서 SVM은 데이터를 linear 하게 분류하는 방법을 보았습니다. 다음엔 Kernel을 이용해 non-linear한 데이터를 분류하는 방법을 보겠습니다.
